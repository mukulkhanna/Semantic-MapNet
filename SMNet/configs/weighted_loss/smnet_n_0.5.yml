name_experiment: weighted_loss_experiments
model:
    arch: smnet
    finetune: False
    n_obj_classes: 13
    ego_feature_dim: 64
    mem_feature_dim: 256
    mem_update: gru
    ego_downsample: False
data:
    train_split: train
    val_split: val
    root: data/training_noise_0.5/
    ego_downsample: False
    feature_type: lastlayer
    gt_semmap_crops: False
training:
    train_epoch: 200
    batch_size: 8
    n_workers: 2
    # loss_weights: [0.001, 0.089, 0.113, 0.018, 0.065, 0.285, 0.027, 0.012, 0.017, 0.044, 0.076, 0.031, 0.221]
    loss_weights: [0.0006, 0.0889, 0.1128, 0.0183, 0.0654, 0.2853, 0.0274, 0.0125, 0.0174, 0.0439, 0.076, 0.0307, 0.2209]
    print_interval: 20
    optimizer:
        lr: 1.0e-4
        momentum: 0.9
        weight_decay: 4.0e-4
    scheduler:
        lr_decay_rate: 0.8
        lr_epoch_per_decay: 30
    resume:
    load_model:
seed: 9876
